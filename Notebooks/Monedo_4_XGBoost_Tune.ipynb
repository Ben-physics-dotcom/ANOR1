{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78df235a",
   "metadata": {},
   "source": [
    "# 1) Read in Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dd8a1f-8431-4f95-b1bb-e1d8700a6bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%run Imports.ipynb\n",
    "name = 'Kred'\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 0.3, 0.5, 1.0],\n",
    "    'max_depth': [5, 7, 10, 15, 20, 50],\n",
    "    'n_estimators': [50, 100, 200, 500, 1000],  # \n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.5, 1.0],\n",
    "    'min_child_weight': [1, 3, 5, 10],\n",
    "    'reg_alpha': [0, 0.01, 0.1, 1, 10, 100],\n",
    "    'reg_lambda': [0, 0.01, 0.1, 1, 10, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9799522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_featsubgroups = pd.read_pickle('../pickle/2_FS/' + name + '/key_featsubgroups.pkl')\n",
    "df = pd.read_pickle('../pickle/2_FS/' + name + '/2_df_new_.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d0f13a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arrears\n",
      "1   0.646\n",
      "0   0.354\n",
      "Name: count, dtype: float64\n",
      "df_shape:  (129457, 418)\n"
     ]
    }
   ],
   "source": [
    "print(df[target].value_counts() / df.shape[0])\n",
    "print('df_shape: ', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33eba05",
   "metadata": {},
   "source": [
    "# 2) Create Model prediction functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f139308d",
   "metadata": {},
   "source": [
    "## 2.1) Split dataset into train/testing while excluding demographic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b80f656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_4(df, key_featsubgroups=key_featsubgroups, target=target, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and testing sets while excluding demographic features.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The dataset containing features and target variable.\n",
    "    key_featsubgroups (DataFrame): A mapping of feature subgroups.\n",
    "    target (str): The name of the target variable.\n",
    "    test_size (float, optional): The proportion of the dataset to allocate for testing. Default is 0.2.\n",
    "    random_state (int, optional): Random seed for reproducibility. Default is 42.\n",
    "\n",
    "    Returns:\n",
    "    tuple: X_train, X_test, y_train, y_test (training and testing datasets)\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract demographic features\n",
    "    demo_feat = key_featsubgroups.loc[key_featsubgroups['subgroup'] == 'demo', 'list_features'].values[0]\n",
    "    print(\"Demographic Features:\", demo_feat)\n",
    "\n",
    "    # Separate features (X) and target variable (y), excluding demographic features\n",
    "    X = df.drop(columns=[target] + demo_feat)\n",
    "    y = df[target]\n",
    "\n",
    "    # Split the dataset into training (80%) and testing (20%) sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Print dataset shapes\n",
    "    print(f\"Training Features Shape: {X_train.shape}\")\n",
    "    print(f\"Training Labels Shape: {y_train.shape}\")\n",
    "    print(f\"Testing Features Shape: {X_test.shape}\")\n",
    "    print(f\"Testing Labels Shape: {y_test.shape}\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b94d2e1",
   "metadata": {},
   "source": [
    "## 2.2) Model training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbdda5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d31d313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50645d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save default_params to a pickle file\n",
    "# with open('../pickle/3_Model/xgb_default_params.pkl', 'wb') as f:\n",
    "#     pickle.dump(default_params, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Save results_dict_updated to a pickle file\n",
    "# with open('../pickle/3_Model/results_dict_updated_pre-Bayes.pkl', 'wb') as f:\n",
    "#     pickle.dump(results_dict_updated, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0e6514f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('../pickle/3_Model/xgb_default_params.pkl', 'rb') as f:\n",
    "    default_params = pickle.load(f)\n",
    "\n",
    "# Load results_dict_updated from the pickle file\n",
    "with open('../pickle/3_Model/results_dict_updated_pre-Bayes.pkl', 'rb') as f:\n",
    "    results_dict_updated = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77387554",
   "metadata": {},
   "source": [
    "# 6a) Run Bayesian hyperparameter optimization for XGBoost using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d936635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_BayesSearch_XGB_Optuna(default_params, param_grid, df, results_dict):\n",
    "    \"\"\"\n",
    "    Performs Bayesian hyperparameter optimization for an XGBoost classifier using Optuna.\n",
    "\n",
    "    This function:\n",
    "      1. Splits the input dataframe into training and testing sets.\n",
    "      2. Determines the number of iterations (n_iter) based on the parameter grid.\n",
    "      3. Unwraps default parameters from lists to their actual values.\n",
    "      4. Defines an objective function for Optuna.\n",
    "      5. Runs the Bayesian optimization using Optuna.\n",
    "      6. Evaluates the optimized model using cross-validation via the model_pred function.\n",
    "      7. Returns the updated results dictionary containing evaluation metrics.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    default_params : dict\n",
    "        Dictionary of default hyperparameters for XGBoost, with each value wrapped in a list.\n",
    "    param_grid : dict\n",
    "        The hyperparameter search space, where each parameter's possible values are provided as a list or distribution.\n",
    "    df : DataFrame\n",
    "        The dataset containing features and the target variable.\n",
    "    results_dict : dict\n",
    "        Dictionary to store the model evaluation results.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    results_dict_updated : dict\n",
    "        The updated results dictionary with model performance metrics.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 1. Split the dataset into training and testing sets.\n",
    "    X_train, X_test, y_train, y_test = split_data_4(df)\n",
    "\n",
    "    # 2. Determine the number of iterations for Bayesian optimization\n",
    "    bcvj = int(np.cumsum([len(values) for values in param_grid.values()])[-1])\n",
    "    print(\"No. of trials: \", bcvj)\n",
    "\n",
    "    # 3. Unwrap default parameters: convert each parameter's value from a list to its actual value.\n",
    "    default_params_xgb = {key: value[0] for key, value in default_params.items()}\n",
    "\n",
    "    # 4. Define the objective function for Optuna\n",
    "    def objective(trial):\n",
    "        # Build the parameters dictionary from param_grid using trial.suggest_categorical for each key\n",
    "        params = {key: trial.suggest_categorical(key, values) for key, values in param_grid.items()}\n",
    "\n",
    "        # Merge with default parameters\n",
    "        # params.update(default_params_xgb)\n",
    "\n",
    "        # Train the XGBoost model\n",
    "        xgbc = xgb.XGBClassifier(**params)\n",
    "\n",
    "        # Evaluate with cross-validation\n",
    "        accuracy = cross_val_score(xgbc, X_train, y_train, cv=5, scoring='accuracy').mean()\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    # 5. Run Bayesian Optimization using Optuna\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=bcvj)\n",
    "\n",
    "    # 6. Train the best model\n",
    "    best_params = study.best_params\n",
    "    # best_params.update(default_params_xgb)\n",
    "    best_xgbc = xgb.XGBClassifier(**best_params)\n",
    "\n",
    "    # 7. Evaluate the final model\n",
    "    results_dict = model_pred(\n",
    "        X_train, X_test, y_train, y_test, best_xgbc, 'xgbc_optuna', 'opt', results_dict\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Optimization completed in {(end_time - start_time) / 60:.2f} minutes\")\n",
    "\n",
    "    return best_params, results_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8d53c0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographic Features: ['clientdata.demo.gender', 'clientdata.demo.age_year', 'clientdata.demo.age_month', 'clientdata.demo.children', 'clientdata.demo.children_singleparent', 'clientdata.demo.maritalstatus_expand_SINGLE', 'clientdata.demo.maritalstatus_expand_MARRIED', 'clientdata.demo.maritalstatus_expand_DIVORCED', 'clientdata.demo.maritalstatus_expand_WIDOWED', 'clientdata.demo.maritalstatus_expand_newvalue', 'clientdata.demo.maritalstatus_woe']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 11:50:28,579] A new study created in memory with name: no-name-f3c4d6e0-54e4-4a30-8ecd-c3a400001e7e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (103565, 406)\n",
      "Training Labels Shape: (103565,)\n",
      "Testing Features Shape: (25892, 406)\n",
      "Testing Labels Shape: (25892,)\n",
      "No. of trials:  43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 11:51:41,413] Trial 0 finished with value: 0.6569304301646309 and parameters: {'learning_rate': 0.01, 'max_depth': 20, 'n_estimators': 50, 'subsample': 0.6, 'colsample_bytree': 0.8, 'gamma': 0.1, 'min_child_weight': 5, 'reg_alpha': 0.1, 'reg_lambda': 0.01}. Best is trial 0 with value: 0.6569304301646309.\n",
      "[I 2025-05-20 11:53:10,867] Trial 1 finished with value: 0.6954183363105295 and parameters: {'learning_rate': 0.1, 'max_depth': 20, 'n_estimators': 200, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 1.0, 'min_child_weight': 10, 'reg_alpha': 0.1, 'reg_lambda': 0}. Best is trial 1 with value: 0.6954183363105295.\n",
      "[I 2025-05-20 11:54:04,915] Trial 2 finished with value: 0.6954859267126925 and parameters: {'learning_rate': 0.5, 'max_depth': 15, 'n_estimators': 500, 'subsample': 0.6, 'colsample_bytree': 1.0, 'gamma': 0.5, 'min_child_weight': 10, 'reg_alpha': 100, 'reg_lambda': 100}. Best is trial 2 with value: 0.6954859267126925.\n",
      "[I 2025-05-20 12:09:09,509] Trial 3 finished with value: 0.6876744073770096 and parameters: {'learning_rate': 0.3, 'max_depth': 50, 'n_estimators': 50, 'subsample': 1.0, 'colsample_bytree': 0.8, 'gamma': 0.2, 'min_child_weight': 1, 'reg_alpha': 0.01, 'reg_lambda': 10}. Best is trial 2 with value: 0.6954859267126925.\n",
      "[I 2025-05-20 12:29:39,455] Trial 4 finished with value: 0.6880702940182495 and parameters: {'learning_rate': 0.01, 'max_depth': 50, 'n_estimators': 200, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0.1, 'min_child_weight': 1, 'reg_alpha': 1, 'reg_lambda': 10}. Best is trial 2 with value: 0.6954859267126925.\n",
      "[I 2025-05-20 12:29:52,727] Trial 5 finished with value: 0.6610148216096172 and parameters: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 0.8, 'gamma': 0.2, 'min_child_weight': 1, 'reg_alpha': 0.01, 'reg_lambda': 0.1}. Best is trial 2 with value: 0.6954859267126925.\n",
      "[I 2025-05-20 12:30:54,677] Trial 6 finished with value: 0.7026601651136968 and parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0.1, 'min_child_weight': 10, 'reg_alpha': 0, 'reg_lambda': 100}. Best is trial 6 with value: 0.7026601651136968.\n",
      "[I 2025-05-20 12:31:16,004] Trial 7 finished with value: 0.6866315840293534 and parameters: {'learning_rate': 1.0, 'max_depth': 10, 'n_estimators': 200, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0.5, 'min_child_weight': 1, 'reg_alpha': 100, 'reg_lambda': 0}. Best is trial 6 with value: 0.7026601651136968.\n",
      "[I 2025-05-20 12:32:17,404] Trial 8 finished with value: 0.6331868874619804 and parameters: {'learning_rate': 1.0, 'max_depth': 20, 'n_estimators': 100, 'subsample': 0.6, 'colsample_bytree': 1.0, 'gamma': 0.5, 'min_child_weight': 3, 'reg_alpha': 0.1, 'reg_lambda': 1}. Best is trial 6 with value: 0.7026601651136968.\n",
      "[I 2025-05-20 12:33:26,214] Trial 9 finished with value: 0.6603002945010379 and parameters: {'learning_rate': 0.01, 'max_depth': 20, 'n_estimators': 50, 'subsample': 1.0, 'colsample_bytree': 1.0, 'gamma': 0.5, 'min_child_weight': 10, 'reg_alpha': 0, 'reg_lambda': 0}. Best is trial 6 with value: 0.7026601651136968.\n",
      "[I 2025-05-20 12:34:35,925] Trial 10 finished with value: 0.7029015594071357 and parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.8, 'colsample_bytree': 0.6, 'gamma': 0, 'min_child_weight': 5, 'reg_alpha': 0, 'reg_lambda': 100}. Best is trial 10 with value: 0.7029015594071357.\n",
      "[I 2025-05-20 12:35:45,647] Trial 11 finished with value: 0.7029015594071357 and parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.8, 'colsample_bytree': 0.6, 'gamma': 0, 'min_child_weight': 5, 'reg_alpha': 0, 'reg_lambda': 100}. Best is trial 10 with value: 0.7029015594071357.\n",
      "[I 2025-05-20 12:37:04,095] Trial 12 finished with value: 0.7028919036353981 and parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.8, 'colsample_bytree': 0.6, 'gamma': 0, 'min_child_weight': 5, 'reg_alpha': 10, 'reg_lambda': 100}. Best is trial 10 with value: 0.7029015594071357.\n",
      "[I 2025-05-20 12:38:31,422] Trial 13 finished with value: 0.6451021097861247 and parameters: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.8, 'colsample_bytree': 0.6, 'gamma': 0, 'min_child_weight': 5, 'reg_alpha': 0, 'reg_lambda': 100}. Best is trial 10 with value: 0.7029015594071357.\n",
      "[W 2025-05-20 12:39:38,652] Trial 14 failed with parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.8, 'colsample_bytree': 0.6, 'gamma': 0, 'min_child_weight': 5, 'reg_alpha': 0, 'reg_lambda': 100} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\benjf\\AppData\\Local\\Temp\\ipykernel_19280\\4044866520.py\", line 54, in objective\n",
      "    accuracy = cross_val_score(xgbc, X_train, y_train, cv=5, scoring='accuracy').mean()\n",
      "  File \"c:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 216, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 684, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "  File \"c:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 216, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 411, in cross_validate\n",
      "    results = parallel(\n",
      "  File \"c:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"c:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\joblib\\parallel.py\", line 1985, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"c:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\joblib\\parallel.py\", line 1913, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"c:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\xgboost\\sklearn.py\", line 1682, in fit\n",
      "    self._Booster = train(\n",
      "  File \"c:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\xgboost\\training.py\", line 183, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"c:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\xgboost\\core.py\", line 2247, in update\n",
      "    _LIB.XGBoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2025-05-20 12:39:38,678] Trial 14 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m best_params, results_dict \u001b[38;5;241m=\u001b[39m \u001b[43mrun_BayesSearch_XGB_Optuna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdefault_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults_dict_updated\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 60\u001b[0m, in \u001b[0;36mrun_BayesSearch_XGB_Optuna\u001b[1;34m(default_params, param_grid, df, results_dict)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# 5. Run Bayesian Optimization using Optuna\u001b[39;00m\n\u001b[0;32m     59\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbcvj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# 6. Train the best model\u001b[39;00m\n\u001b[0;32m     63\u001b[0m best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n",
      "File \u001b[1;32mc:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[7], line 54\u001b[0m, in \u001b[0;36mrun_BayesSearch_XGB_Optuna.<locals>.objective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     51\u001b[0m xgbc \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Evaluate with cross-validation\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxgbc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n",
      "File \u001b[1;32mc:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:684\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    682\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 684\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:411\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    410\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 411\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    431\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\joblib\\parallel.py:1985\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1983\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1984\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1987\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1988\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1989\u001b[0m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1990\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1991\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1992\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\joblib\\parallel.py:1913\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1911\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1912\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1913\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1914\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:866\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    864\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    865\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 866\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\xgboost\\sklearn.py:1682\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1660\u001b[0m model, metric, params, feature_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(\n\u001b[0;32m   1661\u001b[0m     xgb_model, params, feature_weights\n\u001b[0;32m   1662\u001b[0m )\n\u001b[0;32m   1663\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1664\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1665\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1679\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[0;32m   1680\u001b[0m )\n\u001b[1;32m-> 1682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1685\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1694\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1697\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\xgboost\\training.py:183\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\benjf\\anaconda3\\envs\\Work\\lib\\site-packages\\xgboost\\core.py:2247\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2246\u001b[0m     _check_call(\n\u001b[1;32m-> 2247\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2248\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[0;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2250\u001b[0m     )\n\u001b[0;32m   2251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2252\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_params, results_dict = run_BayesSearch_XGB_Optuna(default_params, param_grid, df, results_dict_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d03eaf9-d549-4f73-9c4e-3ed6d00f248e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save results_dict_updated to a pickle file\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../pickle/4_Model_Optuna/results_dict_updated.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 3\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(\u001b[43mresults_dict\u001b[49m, f, pickle\u001b[38;5;241m.\u001b[39mHIGHEST_PROTOCOL)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results_dict' is not defined"
     ]
    }
   ],
   "source": [
    "# Save results_dict_updated to a pickle file\n",
    "with open('../pickle/4_Model_Optuna/results_dict_updated.pkl', 'wb') as f:\n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729d3649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_best_XGB(df, best_params, results_dict):\n",
    "    \"\"\"\n",
    "    Trains an XGBoost model using the best parameters found from Optuna with CV=5,\n",
    "    evaluates performance, and stores results in the given dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        The dataset containing features and the target variable.\n",
    "    best_params : dict\n",
    "        Best hyperparameters found from Optuna optimization.\n",
    "    results_dict : dict\n",
    "        Dictionary to store model performance metrics.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Updated results dictionary containing model evaluation metrics.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(\"\\nStarting XGBoost model training and evaluation...\")\n",
    "\n",
    "    # Split dataset\n",
    "    X_train, X_test, y_train, y_test = split_data_4(df)\n",
    "\n",
    "    # Initialize and train XGBoost with best parameters\n",
    "    xgbc = xgb.XGBClassifier(**best_params)\n",
    "    xgbc.fit(X_train, y_train)\n",
    "\n",
    "    # Perform cross-validation with CV=5\n",
    "    y_train_pred = cross_val_predict(xgbc, X_train, y_train, cv=5)\n",
    "\n",
    "    # Make predictions on test set\n",
    "    y_test_pred = xgbc.predict(X_test)\n",
    "\n",
    "    # Compute confusion matrices\n",
    "    cfm_train = confusion_matrix(y_train, y_train_pred)\n",
    "    cfm_test = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "    # Compute accuracy scores\n",
    "    accs_train = accuracy_score(y_train, y_train_pred)\n",
    "    accs_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    # Compute F1-scores for both classes (0 and 1)\n",
    "    f1s_train_p1 = f1_score(y_train, y_train_pred, pos_label=1)\n",
    "    f1s_train_p0 = f1_score(y_train, y_train_pred, pos_label=0)\n",
    "    f1s_test_p1 = f1_score(y_test, y_test_pred, pos_label=1)\n",
    "    f1s_test_p0 = f1_score(y_test, y_test_pred, pos_label=0)\n",
    "\n",
    "    # Compute ROC-AUC score for test data\n",
    "    test_ras = roc_auc_score(y_test, xgbc.predict_proba(X_test)[:, 1])\n",
    "\n",
    "    # Calculate total runtime in minutes\n",
    "    total_time = (time.time() - start_time) / 60\n",
    "    print(f\"XGBoost Model training completed in {total_time:.2f} minutes\")\n",
    "\n",
    "    # Store computed values in results dictionary\n",
    "    results_dict[\"xgbc_optimized\"] = {\n",
    "        \"classifier\": deepcopy(xgbc),\n",
    "        \"cfm_train\": cfm_train,\n",
    "        \"cfm_test\": cfm_test,\n",
    "        \"train_accuracy\": accs_train,\n",
    "        \"test_accuracy\": accs_test,\n",
    "        \"train F1-score label 1\": f1s_train_p1,\n",
    "        \"train F1-score label 0\": f1s_train_p0,\n",
    "        \"test F1-score label 1\": f1s_test_p1,\n",
    "        \"test F1-score label 0\": f1s_test_p0,\n",
    "        \"test roc auc score\": test_ras,\n",
    "        \"best_params\": best_params,\n",
    "        \"time_m\": total_time\n",
    "    }\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d679765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting XGBoost model training and evaluation...\n",
      "Demographic Features: ['clientdata.demo.gender', 'clientdata.demo.age_year', 'clientdata.demo.age_month', 'clientdata.demo.children', 'clientdata.demo.children_singleparent', 'clientdata.demo.maritalstatus_expand_SINGLE', 'clientdata.demo.maritalstatus_expand_MARRIED', 'clientdata.demo.maritalstatus_expand_DIVORCED', 'clientdata.demo.maritalstatus_expand_WIDOWED', 'clientdata.demo.maritalstatus_expand_newvalue', 'clientdata.demo.maritalstatus_woe']\n",
      "Training Features Shape: (103565, 406)\n",
      "Training Labels Shape: (103565,)\n",
      "Testing Features Shape: (25892, 406)\n",
      "Testing Labels Shape: (25892,)\n",
      "XGBoost Model training completed in 8.16 minutes\n"
     ]
    }
   ],
   "source": [
    "results_dict = train_best_XGB(df, best_params, results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eea481c-96eb-4736-aa8b-8ce0bfea1b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results_dict_updated to a pickle file\n",
    "with open('./pickle/4_Model_Optuna/results_dict_updated.pkl', 'wb') as f:\n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0ea218",
   "metadata": {},
   "source": [
    "# 7) Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ed955a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rf0': {'best_params': {'bootstrap': True,\n",
      "                         'ccp_alpha': 0.0,\n",
      "                         'class_weight': None,\n",
      "                         'criterion': 'gini',\n",
      "                         'max_depth': None,\n",
      "                         'max_features': 'sqrt',\n",
      "                         'max_leaf_nodes': None,\n",
      "                         'max_samples': None,\n",
      "                         'min_impurity_decrease': 0.0,\n",
      "                         'min_samples_leaf': 1,\n",
      "                         'min_samples_split': 2,\n",
      "                         'min_weight_fraction_leaf': 0.0,\n",
      "                         'monotonic_cst': None,\n",
      "                         'n_estimators': 100,\n",
      "                         'n_jobs': None,\n",
      "                         'oob_score': False,\n",
      "                         'random_state': 42,\n",
      "                         'verbose': 0,\n",
      "                         'warm_start': False},\n",
      "         'cfm_test': array([[ 2704,  6430],\n",
      "       [ 1637, 15121]]),\n",
      "         'cfm_train': array([[36755,     0],\n",
      "       [    0, 66810]]),\n",
      "         'classifier': RandomForestClassifier(random_state=42),\n",
      "         'test F1-score label 0': 0.40133580705009275,\n",
      "         'test F1-score label 1': 0.7894228510271738,\n",
      "         'test roc auc score': 0.7053104690260587,\n",
      "         'test_accuracy': 0.6884365827282558,\n",
      "         'time_m': 1.36675816377004,\n",
      "         'train F1-score label 0': 1.0,\n",
      "         'train F1-score label 1': 1.0,\n",
      "         'train_accuracy': 1.0},\n",
      " 'xgbc0': {'best_params': {'base_score': None,\n",
      "                           'booster': None,\n",
      "                           'callbacks': None,\n",
      "                           'colsample_bylevel': None,\n",
      "                           'colsample_bynode': None,\n",
      "                           'colsample_bytree': None,\n",
      "                           'device': None,\n",
      "                           'early_stopping_rounds': None,\n",
      "                           'enable_categorical': False,\n",
      "                           'eval_metric': None,\n",
      "                           'feature_types': None,\n",
      "                           'gamma': None,\n",
      "                           'grow_policy': None,\n",
      "                           'importance_type': None,\n",
      "                           'interaction_constraints': None,\n",
      "                           'learning_rate': None,\n",
      "                           'max_bin': None,\n",
      "                           'max_cat_threshold': None,\n",
      "                           'max_cat_to_onehot': None,\n",
      "                           'max_delta_step': None,\n",
      "                           'max_depth': None,\n",
      "                           'max_leaves': None,\n",
      "                           'min_child_weight': None,\n",
      "                           'missing': nan,\n",
      "                           'monotone_constraints': None,\n",
      "                           'multi_strategy': None,\n",
      "                           'n_estimators': None,\n",
      "                           'n_jobs': None,\n",
      "                           'num_parallel_tree': None,\n",
      "                           'objective': 'binary:logistic',\n",
      "                           'random_state': 42,\n",
      "                           'reg_alpha': None,\n",
      "                           'reg_lambda': None,\n",
      "                           'sampling_method': None,\n",
      "                           'scale_pos_weight': None,\n",
      "                           'subsample': None,\n",
      "                           'tree_method': None,\n",
      "                           'validate_parameters': None,\n",
      "                           'verbosity': None},\n",
      "           'cfm_test': array([[ 3701,  5433],\n",
      "       [ 2358, 14400]]),\n",
      "           'cfm_train': array([[20750, 16005],\n",
      "       [ 5400, 61410]]),\n",
      "           'classifier': GridSearchCV(cv=3,\n",
      "             estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                     callbacks=None, colsample_bylevel=None,\n",
      "                                     colsample_bynode=None,\n",
      "                                     colsample_bytree=None, device=None,\n",
      "                                     early_stopping_rounds=None,\n",
      "                                     enable_categorical=False, eval_metric=None,\n",
      "                                     feature_types=None, gamma=None,\n",
      "                                     grow_policy=None, importance_type=None,\n",
      "                                     interaction_constraints=None,\n",
      "                                     learning_rate=None,...\n",
      "                         'max_cat_threshold': [None],\n",
      "                         'max_cat_to_onehot': [None], 'max_delta_step': [None],\n",
      "                         'max_depth': [None], 'max_leaves': [None],\n",
      "                         'min_child_weight': [None], 'missing': [nan],\n",
      "                         'monotone_constraints': [None],\n",
      "                         'multi_strategy': [None], 'n_estimators': [None],\n",
      "                         'n_jobs': [None], 'num_parallel_tree': [None],\n",
      "                         'objective': ['binary:logistic'], ...},\n",
      "             return_train_score=True, scoring='accuracy', verbose=1),\n",
      "           'test F1-score label 0': 0.4871980517343513,\n",
      "           'test F1-score label 1': 0.7870787898663606,\n",
      "           'test roc auc score': 0.7225894358603924,\n",
      "           'test_accuracy': 0.6990962459446933,\n",
      "           'time_m': 0.1614015499750773,\n",
      "           'train F1-score label 0': 0.659724982115889,\n",
      "           'train F1-score label 1': 0.8515860634425377,\n",
      "           'train_accuracy': 0.7933182059576112},\n",
      " 'xgbc_optimized': {'best_params': {'colsample_bytree': 0.6,\n",
      "                                    'gamma': 0.2,\n",
      "                                    'learning_rate': 0.01,\n",
      "                                    'max_depth': 50,\n",
      "                                    'min_child_weight': 10,\n",
      "                                    'n_estimators': 1000,\n",
      "                                    'reg_alpha': 0.01,\n",
      "                                    'reg_lambda': 10,\n",
      "                                    'subsample': 0.6},\n",
      "                    'cfm_test': array([[ 3467,  5667],\n",
      "       [ 1992, 14766]]),\n",
      "                    'cfm_train': array([[14107, 22648],\n",
      "       [ 7858, 58952]]),\n",
      "                    'classifier': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.6, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=0.2, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=50, max_leaves=None,\n",
      "              min_child_weight=10, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=1000, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...),\n",
      "                    'test F1-score label 0': 0.47515932296306446,\n",
      "                    'test F1-score label 1': 0.7940630797773655,\n",
      "                    'test roc auc score': 0.7373118651153623,\n",
      "                    'test_accuracy': 0.7041943457438591,\n",
      "                    'time_m': 8.156399281819661,\n",
      "                    'train F1-score label 0': 0.48048365122615805,\n",
      "                    'train F1-score label 1': 0.794447813489657,\n",
      "                    'train_accuracy': 0.7054410273741129},\n",
      " 'xgbc_optuna': {'best_params': {'base_score': None,\n",
      "                                 'booster': None,\n",
      "                                 'callbacks': None,\n",
      "                                 'colsample_bylevel': None,\n",
      "                                 'colsample_bynode': None,\n",
      "                                 'colsample_bytree': 0.6,\n",
      "                                 'device': None,\n",
      "                                 'early_stopping_rounds': None,\n",
      "                                 'enable_categorical': False,\n",
      "                                 'eval_metric': None,\n",
      "                                 'feature_types': None,\n",
      "                                 'gamma': 0.2,\n",
      "                                 'grow_policy': None,\n",
      "                                 'importance_type': None,\n",
      "                                 'interaction_constraints': None,\n",
      "                                 'learning_rate': 0.01,\n",
      "                                 'max_bin': None,\n",
      "                                 'max_cat_threshold': None,\n",
      "                                 'max_cat_to_onehot': None,\n",
      "                                 'max_delta_step': None,\n",
      "                                 'max_depth': 50,\n",
      "                                 'max_leaves': None,\n",
      "                                 'min_child_weight': 10,\n",
      "                                 'missing': nan,\n",
      "                                 'monotone_constraints': None,\n",
      "                                 'multi_strategy': None,\n",
      "                                 'n_estimators': 1000,\n",
      "                                 'n_jobs': None,\n",
      "                                 'num_parallel_tree': None,\n",
      "                                 'objective': 'binary:logistic',\n",
      "                                 'random_state': None,\n",
      "                                 'reg_alpha': 0.01,\n",
      "                                 'reg_lambda': 10,\n",
      "                                 'sampling_method': None,\n",
      "                                 'scale_pos_weight': None,\n",
      "                                 'subsample': 0.6,\n",
      "                                 'tree_method': None,\n",
      "                                 'validate_parameters': None,\n",
      "                                 'verbosity': None},\n",
      "                 'cfm_test': array([[ 3467,  5667],\n",
      "       [ 1992, 14766]]),\n",
      "                 'cfm_train': array([[31093,  5662],\n",
      "       [  783, 66027]]),\n",
      "                 'classifier': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.6, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=0.2, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=50, max_leaves=None,\n",
      "              min_child_weight=10, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=1000, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...),\n",
      "                 'test F1-score label 0': 0.47515932296306446,\n",
      "                 'test F1-score label 1': 0.7940630797773655,\n",
      "                 'test roc auc score': 0.7373118651153623,\n",
      "                 'test_accuracy': 0.7041943457438591,\n",
      "                 'time_m': 1.7822603980700176,\n",
      "                 'train F1-score label 0': 0.9060919992423249,\n",
      "                 'train F1-score label 1': 0.9534653679809962,\n",
      "                 'train_accuracy': 0.9377685511514507}}\n"
     ]
    }
   ],
   "source": [
    "pprint(results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7c54df",
   "metadata": {},
   "source": [
    "# 7) Compile results: AUC, Accuracy and Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cec2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark RandomForest - AUC/accuracy score: 0.7053 / 0.6884 @ 1.37 minutes\n",
      "Benchmark XGBoost - AUC/accuracy score: 0.7226 / 0.6991 @ 0.16 minutes\n",
      "XGBoost w/ Optuna - AUC/accuracy score: 0.7373 / 0.7042 @ 1.78 minutes\n",
      "Optimized XGBoost w/ Optuna - AUC/accuracy score: 0.7373 / 0.7042 @ 8.16 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#printing benchmark, iterative grid search and randomized search ROC AUC / accuracy scores (test data set)\n",
    "print(f\"Benchmark RandomForest - AUC/accuracy score: {np.round(results_dict['rf0']['test roc auc score'],4)} / {np.round(results_dict['rf0']['test_accuracy'],4)} @ {np.round(results_dict['rf0']['time_m'],2)} minutes\")\n",
    "print(f\"Benchmark XGBoost - AUC/accuracy score: {np.round(results_dict['xgbc0']['test roc auc score'],4)} / {np.round(results_dict['xgbc0']['test_accuracy'],4)} @ {np.round(results_dict['xgbc0']['time_m'],2)} minutes\")\n",
    "print(f\"XGBoost w/ Optuna - AUC/accuracy score: {np.round(results_dict['xgbc_optuna']['test roc auc score'],4)} / {np.round(results_dict['xgbc_optuna']['test_accuracy'],4)} @ {np.round(results_dict['xgbc_optuna']['time_m'],2)} minutes\")\n",
    "print(f\"Optimized XGBoost w/ Optuna - AUC/accuracy score: {np.round(results_dict['xgbc_optimized']['test roc auc score'],4)} / {np.round(results_dict['xgbc_optimized']['test_accuracy'],4)} @ {np.round(results_dict['xgbc_optimized']['time_m'],2)} minutes\")\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acef8e0e-e6e6-4fc7-97c2-88eaf7292e81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
