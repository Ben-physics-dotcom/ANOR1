{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29f78075",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b81373b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Imports.ipynb\n",
    "name = 'Kred' # Choose Kred or Mone\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fce70db",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff23aabf",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../pickle/Monedo_5/wo_optuna/ebm_best_results.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../../pickle/Monedo_5/wo_optuna/ebm_best_results.pkl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      2\u001b[39m     ebm_best_res = pickle.load(f)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\benjf\\anaconda3\\envs\\Work\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:327\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    322\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    325\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../../pickle/Monedo_5/wo_optuna/ebm_best_results.pkl'"
     ]
    }
   ],
   "source": [
    "with open(\"../../pickle/Monedo_5/wo_optuna/ebm_best_results.pkl\", \"rb\") as f:\n",
    "    ebm_best_res = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b823aa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../pickle/Monedo_7/without_optuna/results_M7_T2.pkl\", \"rb\") as f:\n",
    "    m7_res = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0be708c",
   "metadata": {},
   "source": [
    "# Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ef3caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Collect predictive performance metrics into a DataFrame\n",
    "metrics = []\n",
    "for method, res in results_dict.items():\n",
    "    # Adapt these keys to whatever metrics you logged:\n",
    "    entry = {\n",
    "        \"method\": method,\n",
    "        \"test_accuracy\": res.get(\"test_accuracy\", None),\n",
    "        \"test_roc_auc\": res.get(\"test_roc_auc\", res.get(\"test roc auc score\", None)),\n",
    "        \"rmse_test\":    res.get(\"rmse_test\", None),\n",
    "        \"mae_test\":     res.get(\"mae_test\", None),\n",
    "        \"r2_test\":      res.get(\"r2_test\", None)\n",
    "    }\n",
    "    metrics.append(entry)\n",
    "perf_df = pd.DataFrame(metrics).set_index(\"method\")\n",
    "print(\"=== Predictive Performance ===\")\n",
    "print(perf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6298d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Aggregate feature importances across methods into a DataFrame\n",
    "#    (only methods that stored \"feature_importances\" or \"global_importance\")\n",
    "fi_dict = {}\n",
    "for method, res in results_dict.items():\n",
    "    if \"feature_importances\" in res:\n",
    "        fi = res[\"feature_importances\"]\n",
    "        fi_dict[method] = fi\n",
    "    elif \"global_importance\" in res:\n",
    "        # EBM’s global_importance is a list of dicts: [{\"feature\": name, \"score\": s}, …]\n",
    "        # Convert to a numeric array aligned by feature index:\n",
    "        gi = pd.Series({d[\"feature\"]: d[\"score\"] for d in res[\"global_importance\"]})\n",
    "        # Sort by feature name to align across methods\n",
    "        fi_dict[method] = gi.sort_index().values\n",
    "\n",
    "# Build DataFrame: rows=features, cols=methods\n",
    "# (assumes all arrays same length and same feature order; otherwise index with feature names)\n",
    "fi_df = pd.DataFrame(fi_dict)\n",
    "print(\"\\n=== Feature Importances ===\")\n",
    "print(fi_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c546403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Compute pairwise Spearman rank correlations\n",
    "corr = fi_df.corr(method=\"spearman\")\n",
    "print(\"\\n=== Spearman Rank-Correlation of Feature Importances ===\")\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783d4f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) If you want a long-format table of correlations:\n",
    "corr_long = corr.reset_index().melt(id_vars=\"index\", var_name=\"method2\", value_name=\"spearman_r\")\n",
    "corr_long.columns = [\"method1\", \"method2\", \"spearman_r\"]\n",
    "print(\"\\n=== Pairwise Spearman Correlations (long format) ===\")\n",
    "print(corr_long)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
